# Flowers classification with tpus

## 数据集来源

Kaggle公开数据集：https://www.kaggle.com/c/flower-classification-with-tpus/data

## 开发过程

1. 自定义网络ZNN，效果很差，且精度和loss波动很大，这个应该是学习率的问题。
2. 使用VGG16与ImageNet预训练参数，首先完全训练，后面冻结VGG，找到最好的冰封点（即：从某一层开始可以训练VGG的参数）
3. 最优化vgg网络的学习率，设计学习率调度函数（简单的在前几轮上调lr，然后从某个epoch开始缓缓下降）
4. 发现数据集极度不平衡，试图在fit时使用class_weight
5. 设计两种版本的class_weight
6. 发现class_weight效果一般，几乎没有提升。又去调参：Dropout率，数据增强方法（加亮度，加饱和度的调整），效果也一般，只能到78%，接近收敛点了。
7. 怀疑是分类器的过拟合，了解到现代分类器常采用全局平均池化特征图，而不是把特征图的每个特征都进行分析。这样减少了参数，抑制了过拟合。用后确实有提升，反复调节dropout，最后得到82%精度，极限了。
8. 试图使用其他预训练模型，采用了一些参考书的建议，使用了Xception和ResNet，发现res效果很好，没怎么调参就直接把我之前的VGG秒杀（85%）。而且训练速度快，应该是和他的学习小路有关系；而Xception不行，效果不好，遂弃用。
9. 与VGG工作类似（2）->（7），得到88%精度，接近收敛点了。
10. 试图增大训练数据集，发现Kaggle讨论区有人公开了牛津大学的花蕊数据集，并且已经做好数据清洗了，我也跟着采用了，提升约2%，达到90%
11. 继续Google，看看还有没有什么办法提升精度。发现一篇论文提到，网络的深度，宽度，以及图片的分辨率都能影响到网络性能。通过寻找这3个参数的最优组合，就能得到最大化网络性能。它便是EfficientNet系列。
12. 我用512x512的分辨率，所以用b7去训练。结果出乎我的预料，居然达到了93.6%（参数还不是最优化的情况下）
13. 这次我没有对ent进行最优学习率的搜索，而是通过前面的经验，沿用了2e-4这个学习率作为初始学习率（vgg和res的opt_lr都差不多是这个），并设计学习率调度函数（cos版和sigmoid版,对比使用后sigmoid更好些），然后，完全训练，再到冻结部分ENET层，找到最好的冰封点，最后95.4%精度，95.6%的召回率(验证集），测试集94.453%。至此，仅使用官方数据集，达到的最大精度。

